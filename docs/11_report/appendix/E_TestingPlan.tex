\section*{Introduction}
\subsection*{Purpose}
This testing plan in accordance to the IEEE Standard for Software Test Documentation (ANSI/IEEE Standard 829-1983), outlines and describes the testing approach and overall framework that will drive the testing of the implementation phase of the project.

The document outlines:
\begin{itemize}
    \item Testing Strategy: structure and descriptions of testing.
    \item Execution Strategy: describes how the test will be performed and processed to analyse defects to the platform, and resolutions to the defects.
    \item Test Management: processing how to deal with testing platforms and events that take place during execution.
\end{itemize}

\subsection*{Audience}
\begin{itemize}
    \item Project members will conduct tasks specified in this document, and provide working updates to it. All members will be accountable for the results.
    \item The project lead plans the testing activities in the overall project schedule, and tracks the performances of the test.
    \item The project supervisor will ensure that the plan is met by the team and provide further test cases if necessary to important functionalities.
\end{itemize}

\subsection*{References}
This document should be read in conjunction with:
\begin{itemize}
    \item Proposal
    \item GitLab repository testing plan
    \item Gantt chart
\end{itemize}

\section*{Objectives and Tasks}
\subsection*{Test Objectives}
The objective of testing is to verify the functionality of the platform is in accordance to the outlines of the proposal. Test execute and verify test scripts, identifies and fixes various levels of defects.

\subsection*{Assumptions}
General:
\begin{itemize}
    \item During the execution of testing, the current project plan acts as a precondition.
    \item Software delivered by from the development side must be in accordance with the development plans so it is functionally usable and in testable units. 
    \item The quality of the development tests are to be performed in the agreed manner and thoroughness.
    \item Testers for each sprints should be available in accordance with the test schedule. 
    \item Defects will be tracked through GitLab. Any defect fixes planned will be shared with Test Team prior to applying the fixes on the Test environment.
    \item There is no environment downtime during test due to outages or defect fixes.
    \item The system will be treated as a black box; if the information shows correctly on device, and in the reports, it will be assumed the program is functioning.
\end{itemize}

UAT:
\begin{itemize}
    \item UAT test execution will be performed by end users and the team will create the UAT script.
\end{itemize}

\section*{Testing Strategy}
There are three key aspects to this version of the platform. For both route calculations and augmented reality implementation, unit and integration testing will be pivotal in ensuring these functions are implemented rigorously. UI testing will mainly focus on unit testing but user acceptance testing will be heavily featured to match the requirements of the stakeholders.

\subsection*{Alpha Testing (Unit Testing)}
The alpha testing will be carried out in-house by the team. The application will be tested when development has reached 70\% or above. The goals of this testing will be to evaluate the quality of the application, finding any bugs, ensuring the product works and is ready to be tested by the users.\\

%Alpha testing can also be carried out by technical experts

Entry Criteria

\begin{itemize}
    \item 70\% â€“ 90\% complete and assurance to go ahead with Alpha Testing
    \item Alpha Test Cases should be designed and reviewed
    \item Testing environment set up and and stability confirmed
\end{itemize}

Exit Criteria

\begin{itemize}
    \item All test cycles should be complete
    \item All the Alpha Tests should be executed and Passed
    \item Alpha version of the application frozen (i.e., no additional features, no modifications to existing features, no dropping of the existing features)
    \item Alpha Test formal Sign-Off
\end{itemize}

\subsection*{System and Integration Testing}
This type of testing will verify the behaviour of the integrated hardware and software environment of the complete system. Helping to evaluate the system's compliance with its specified requirements.\\

Integration will be tested using incremental testing, taking on the 'top down' approach. First testing each module of the application individually and then continue testing by considering other modules in addition. As the nature of the application is an Augmented Reality navigation app, testing the Bluetooth functionality prior to testing the AR functionality is one example of how integration testing would occur. This would be followed by testing of both these modules combined.

\subsection*{Performance and Stress Testing} %https://www.blazemeter.com/blog/performance-testing-vs-load-testing-vs-stress-testing
Performance testing examines responsiveness, stability, scalability, reliability, speed and resource usage of the software and infrastructure. As development of the application will be done under the agile methodology, continuous testing will be carried out to assess the performance of the application.\\

Stress testing will be carried out to check the upper limits of the application, testing it under extreme loads. As the software developed will be an application used for navigational purposes in a commercial space, an example definition of a test case would be with a very high number of users, which is known as a 'Spike Test'.

\subsection*{User Acceptance Testing (UAT)} %https://www.softwaretestinghelp.com/what-is-user-acceptance-testing-uat/
UAT will be performed at the very end of development, prior to the product going live. This testing will be carried out by real users who will decide whether or not the acceptance requirements provided by the team are to be accepted or rejected. One example of an acceptance requirement would be "the route calculation must be accurate". This is an optimal phase for also identifying any bugs.\\

Acceptance criteria will be gathered by the team with real users comparing the system to the initial requirements. During testing, the team will \textbf{Assist In UAT}, who will be on stand-by to help users in the case of any difficulty. However, the main reason for the team to be on stand-by is to record results and log any bugs etc.

%A complication with UAT is that it can be classified as both Alpha and Beta testing..

\subsection*{Beta Testing}
Beta testing %which is very similar to UAT
is the final stage of the testing phase, where the application will be released to an external test group consisting of real users. The main entry criteria being that the development should be 90\% - 95\% completed, all components either fully or almost complete for testing. At this point in testing, the Alpha Testing should be signed off. Any bugs identified will be handled promptly and feedback analysed to ensure application satisfies the user. Test cases written in this phase will be clearly outlined, defining which feature is being examined, such as UI, Bluetooth recognition, location handling, route calculations etc.

%However in the case of time constraints, beta testing might be carried out by stakeholders or companies such as Fuse etc. (instead of real users)

\subsection*{Validation and Defect Management}
\begin{itemize}
    \item It is the responsibility of testers to open defects and link them to the corresponding code, and assign an initial severity and status, before retesting and closing the defect. It is the responsibility of the project lead to ensure the defects are fixed in a timely manner and according to the project and testing plan. 
\end{itemize}

Severity categories (from softwaretestinghelp.com):
\begin{enumerate}
    \item Critical - The bug is critical enough to crash the system, or cause potential data loss. It causes an abnormal return to the operating system. Or it causes the application the hang and requires a re-boot of the system.
    \item High - It causes a lack of vital program functionality with workaround
    \item Medium - This Bug will degrade the quality of the System. However there is an intelligent workaround for achieving the desired functionality. Or this bug prevents other areas of the product from being tested. However other areas can be independently tested.
    \item Low - There is an insufficient or unclear error message, which has minimum impact on product use.
    \item Cosmetic - There is an insufficient or unclear error message that has no impact on the product use.
\end{enumerate}

Testing metrics allows the measurements and level of success of test that will be developed with the project lead.
\begin{itemize}
    \item Test preparation and execution status - To report on \% complete [daily/weekly]
    \item Daily execution - To report on pass, fail, total defects, critical defects [daily]
    \item Project weekly status - Project driven reporting (as requested by supervisor) [weekly]
\end{itemize}

\section*{Hardware Requirements}
\begin{itemize}
    \item Raspberry Pi with Bluetooth beacon
    \item Mobile device with Bluetooth beacon
    \item Android device with Android 5.0 (Lollipop) or higher
\end{itemize}

\section*{Test Schedule}
Tests will be executed during each sprints as outlined in the Gantt chart, and a final testing stage will be completed for the testing milestone.

\section*{Control Procedures}
\subsection*{Problem Reporting}
If there are defective software found during testing, the project lead will assign the defect to the team, and fix it before it is sent back to testing. Project lead will require approval to ensure the updated software matches the requirements of the test.

\subsection*{Change Requests}
If modifications to the software are made, the project lead is required to sign off the changes and review the changes to the current platform. If there are changes that will affect the existing platform, then these particular modules will have to be identified.

\section*{Features to Be Tested}
To be read in conjunction to the backlog:
\begin{itemize}
    \item Receiving user inputs for user destination
    \item Route calculations
    \item Superimposing 3D directional line
    \item Display navigation
\end{itemize}

\section*{Features Not to Be Tested}
These features will appear in later iterations. This is due to the short implementation time available for the project.
\begin{itemize}
    \item Finding museums nearby
    \item Mobile device camera recognition
    \item Receiving and displaying information about the exhibit
    \item Rating and dealing with user reviews of platform
\end{itemize}

\section*{Risks/Assumptions}

\section*{Tools}
All tests will be mainly conducted on the Android Studio testing suite. JUnit will specifically conduct unit testing, and GitLab will have continuous integration and continuous delivery in order to ensure integration to the current platform is successful.

All testing artifacts such as the test cases themselves are stored on the GitLab repository.

All tests should be tested on devices higher than Android 5.0 (Lollipop) that have allowed Bluetooth to be used.

